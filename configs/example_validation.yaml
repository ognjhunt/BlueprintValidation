schema_version: v1
project_name: "Blueprint Validation Pilot"

# ---- Facilities ----
facilities:
  facility_a:
    name: "Warehouse A - Durham"
    ply_path: ../data/facilities/facility_a/splat.ply
    # Optional: BlueprintCapturePipeline task_targets.json generated from source video.
    # When present, Stage 4 auto-injects additional manipulation/articulation tasks.
    # task_hints_path: /Users/nijelhunt_1/workspace/BlueprintCapturePipeline/runs/<run>/task_targets.json
    description: "Large warehouse with shelving racks, loading dock, concrete floors"
    landmarks:
      - "loading dock on east wall"
      - "shelving racks in center"
      - "break room in northwest corner"
    floor_height_m: 0.0
    ceiling_height_m: 6.0
    # Scene orientation â€” default "auto" detects from point cloud extents.
    # Override: "z", "y", "-y", "-z", "x", "-x"
    # up_axis: "auto"
    # scene_rotation_deg: [0, 0, 180]  # additional XYZ rotation in degrees
    manipulation_zones:
      - name: shelf_tote_pick
        approach_point: [2.1, 0.5, 0.8]
        target_point: [2.1, 1.5, 0.3]
        camera_height_m: 0.6
        camera_look_down_deg: 45
        arc_radius_m: 0.4
      - name: packing_station_place
        approach_point: [5.0, 0.0, 0.9]
        target_point: [5.0, 0.5, 0.9]

  facility_b:
    name: "Warehouse B - Raleigh"
    ply_path: ../data/facilities/facility_b/splat.ply
    # task_hints_path: /Users/nijelhunt_1/workspace/BlueprintCapturePipeline/runs/<run>/task_targets.json
    description: "Smaller warehouse with conveyor system, narrow aisles"
    landmarks:
      - "conveyor belt along south wall"
      - "narrow aisles between racks"
      - "office area on second floor"
    floor_height_m: 0.0
    ceiling_height_m: 5.0
    manipulation_zones:
      - name: conveyor_place
        approach_point: [4.0, 0.0, 0.9]
        target_point: [4.0, 0.5, 0.9]

# ---- Stage 1: Render ----
render:
  resolution: [480, 640]
  fps: 10
  num_frames: 49
  camera_height_m: 1.2
  camera_look_down_deg: 15
  camera_paths:
    - type: orbit
      radius_m: 3.0
      num_orbits: 2
    - type: sweep
      length_m: 10.0
    # Close-range arc around a tote/bin manipulation zone (gripper-centric view).
    - type: file
      path: ./camera_paths/manipulation_closeup_arc.json
    # Auto-generated gripper-height arc for each manipulation zone.
    - type: manipulation
      height_override_m: 0.6
      look_down_override_deg: 45
  num_clips_per_path: 3
  # Scene-aware camera placement: uses task_targets.json OBBs (or VLM fallback)
  # to generate extra camera paths centered on detected objects.
  scene_aware: true
  # Collision avoidance via voxelized Gaussian point cloud occupancy grid.
  collision_check: true
  voxel_size_m: 0.1
  density_threshold: 3
  min_clearance_m: 0.15
  # VLM fallback: when no task_targets.json exists, render overview frames and
  # send to Gemini for object detection + depth unprojection to 3D positions.
  vlm_fallback: true
  vlm_fallback_model: gemini-3-flash-preview
  vlm_fallback_num_views: 4

# ---- Stage 1b: Robot arm compositing (URDF + camera extrinsics) ----
robot_composite:
  enabled: true
  urdf_path: ./robots/sample_6dof_arm.urdf
  end_effector_link: wrist_3_link
  base_xyz: [0.0, 0.0, 0.0]
  base_rpy: [0.0, 0.0, 0.0]
  # Example start/end joint positions (radians) for pick-to-place style sweep
  start_joint_positions: [0.0, -1.1, 1.2, -1.0, -1.2, 0.0]
  end_joint_positions: [0.4, -0.8, 1.0, -1.2, -1.0, 0.3]
  min_visible_joint_ratio: 0.6
  min_consistency_score: 0.6
  line_color_bgr: [50, 180, 255]
  line_thickness: 3

# ---- Stage 1c: Gemini image polish ----
gemini_polish:
  enabled: true
  model: gemini-3.1-flash-image-preview
  api_key_env: GOOGLE_GENAI_API_KEY
  prompt: "Preserve robot arm pose and scene geometry exactly. Improve photorealism and blending quality."
  sample_every_n_frames: 2

# ---- Stage 1d: Full RoboSplat default augmentation ----
robosplat:
  enabled: true
  backend: auto                  # auto|vendor|native|legacy_scan
  parity_mode: hybrid            # hybrid|strict|scan_only
  runtime_preset: balanced       # balanced|high_quality|fast
  variants_per_input: 4
  object_source_priority: [task_hints_obb, vlm_detect, cluster]
  demo_source: synthetic         # synthetic|real|required_real
  bootstrap_if_missing_demo: true
  bootstrap_num_rollouts: 6
  bootstrap_horizon_steps: 24
  bootstrap_tasks_limit: 4
  quality_gate_enabled: true
  min_variants_required_per_clip: 1
  fallback_to_legacy_scan: true
  fallback_on_backend_error: true
  persist_scene_variants: false
  vendor_repo_path: ../vendor/robosplat
  vendor_ref: ""

# ---- Stage 1e: Minimal SplatSim interaction (opt-in) ----
splatsim:
  enabled: false
  mode: hybrid                   # hybrid|strict
  per_zone_rollouts: 2
  horizon_steps: 30
  min_successful_rollouts_per_zone: 1
  fallback_to_prior_manifest: true

# ---- Legacy Stage 1d compatibility (one release cycle) ----
robosplat_scan:
  enabled: true
  num_augmented_clips_per_input: 2
  yaw_jitter_deg: 6.0
  pitch_jitter_deg: 4.0
  camera_height_jitter_m: 0.12
  relight_gain_min: 0.85
  relight_gain_max: 1.20
  color_temp_shift: true
  temporal_speed_factors: [0.9, 1.1]

# ---- Stage 2: Enrich ----
enrich:
  cosmos_model: nvidia/Cosmos-Transfer2.5-2B
  cosmos_checkpoint: ../data/checkpoints/cosmos-transfer-2.5-2b/
  cosmos_repo: /opt/cosmos-transfer
  controlnet_inputs: [rgb, depth]
  num_variants_per_render: 5
  guidance: 7.0
  # Dynamic variants: Gemini analyzes a sample rendered frame and generates
  # scene-appropriate prompts (e.g. lighting, weather, activity variations).
  # Falls back to builtin warehouse-style variants if Gemini is unavailable.
  dynamic_variants: true
  dynamic_variants_model: gemini-3-flash-preview
  # Override with explicit variants if you want full control:
  # variants:
  #   - name: daylight_empty
  #     prompt: "Bright daylight, empty clean industrial space, polished concrete floors"
  #   - name: evening_dim
  #     prompt: "Dim evening lighting, overhead fluorescents casting shadows"

# ---- Stage 3: Fine-tune ----
finetune:
  dreamdojo_repo: /opt/DreamDojo
  dreamdojo_checkpoint: ../data/checkpoints/DreamDojo/2B_pretrain/
  # Optional DreamDojo config script path or config name (without .sh) under dreamdojo_repo/configs.
  # experiment_config: post-training/2b_720p_adapted_17f
  model_size: 2B
  # LoRA via Cosmos config system (model.config.train_architecture=lora)
  use_lora: true
  lora_rank: 32
  lora_alpha: 32
  lora_target_modules: "q_proj,k_proj,v_proj,output_proj,mlp.layer1,mlp.layer2"
  learning_rate: 1.0e-4
  num_epochs: 50
  batch_size: 1
  gradient_accumulation_steps: 4
  warmup_steps: 100
  save_every_n_epochs: 10
  max_training_hours: 72

# ---- Stage 4: OpenVLA-OFT Policy Eval ----
eval_policy:
  model_name: openvla/openvla-7b
  checkpoint_path: ../data/checkpoints/openvla-7b/
  unnorm_key: bridge_orig
  num_rollouts: 50
  max_steps_per_rollout: 100
  conditions: ["baseline", "adapted"]  # "trained" added automatically when S3b succeeds
  min_absolute_difference: 0.5  # minimum raw score difference (0-10 scale) for PASS verdict
  tasks:
    - "Navigate forward through the corridor"
    - "Turn left at the intersection"
    - "Approach the nearest obstacle"
  manipulation_tasks:
    - "Approach the tote on the floor pallet"
    - "Align gripper with tote handles"
    - "Lift the tote to shelf level one"
    - "Place the tote onto the target shelf"
    - "Pick from left bin and place into right bin"
    - "Regrasp the tote and stabilize before placement"
  vlm_judge:
    model: gemini-3-flash-preview
    api_key_env: GOOGLE_GENAI_API_KEY
    enable_agentic_vision: true

# ---- Stage 3b: OpenVLA-OFT policy fine-tune ----
# Uses OFT-oriented finetuning defaults (OpenVLA-OFT-style recipe).
# Trains policy on site-specific rollout data from Stage 4a (runs in world model, not IRL).
policy_finetune:
  enabled: true
  openvla_repo: /opt/openvla-oft
  finetune_script: vla-scripts/finetune.py
  # data_root_dir should contain datasets like <data_root_dir>/bridge_orig
  data_root_dir: ../data/openvla_datasets
  dataset_name: bridge_orig
  recipe: oft
  action_chunk_size: 8
  parallel_decoding: true
  use_continuous_actions: true
  use_l1_regression: true
  lora_rank: 32
  batch_size: 8
  grad_accumulation_steps: 2
  learning_rate: 5.0e-4
  save_steps: 1000
  max_steps: 5000
  image_aug: true
  nproc_per_node: 1

# ---- Stage 3c: World-VLA-Loop-style policy RL loop ----
policy_rl_loop:
  enabled: false
  iterations: 2
  horizon_steps: 24
  rollouts_per_task: 8
  group_size: 4
  reward_mode: hybrid
  vlm_reward_fraction: 0.25
  top_quantile: 0.30
  near_miss_min_quantile: 0.30
  near_miss_max_quantile: 0.60
  policy_refine_steps_per_iter: 1000
  world_model_refresh_enabled: true
  world_model_refresh_epochs: 3
  world_model_refresh_learning_rate: 5.0e-5

# ---- Policy adapter (multi-policy interface) ----
policy_adapter:
  name: openvla_oft
  openvla:
    openvla_repo: /opt/openvla-oft
    finetune_script: vla-scripts/finetune.py
    extra_train_args: []
  pi05:
    openpi_repo: /opt/openpi
    profile: pi05_libero
    runtime_mode: inprocess
    train_backend: pytorch
    train_script: scripts/train_pytorch.py
    norm_stats_script: scripts/compute_norm_stats.py
    policy_action_dim: 7
    policy_state_dim: 7
    extra_train_args: []

# pi0.5 switch example (required refs; preflight will fail if OpenVLA-like refs remain):
# policy_adapter:
#   name: pi05
#   pi05:
#     openpi_repo: /opt/openpi
#     profile: pi05_libero
# eval_policy:
#   model_name: openpi/pi05
#   checkpoint_path: ../data/checkpoints/pi05/
# Also install: `uv sync --extra pi05` (or `pip install -U lerobot`)

# ---- Stage 4b: Rollout -> RLDS-style dataset export ----
rollout_dataset:
  enabled: true
  seed: 17
  train_split: 0.8
  min_steps_per_rollout: 4
  task_score_threshold: 7.0
  include_failed_rollouts: false
  max_action_delta_norm: 5.0
  require_consistent_action_dim: true
  baseline_dataset_name: bridge_dataset
  adapted_dataset_name: bridge_orig
  export_dir: ../data/outputs/policy_datasets

# ---- Stage 4d: Trained policy A/B comparison ----
policy_compare:
  enabled: false
  heldout_num_rollouts: 20
  heldout_seed: 123
  eval_world_model: adapted
  # Optional explicit heldout-task set for pair evaluation.
  heldout_tasks:
    - "Pick up the tote from the shelf"
    - "Place the tote onto the target shelf"
  task_score_success_threshold: 7.0
  manipulation_task_keywords: [pick, grasp, lift, place, stack, regrasp, tote, bin]
  require_grasp_for_manipulation: true
  require_lift_for_manipulation: true
  require_place_for_manipulation: true

# ---- Stage 5: Visual Fidelity ----
eval_visual:
  metrics: [psnr, ssim, lpips]
  lpips_backbone: alex

# ---- Stage 6: Spatial Accuracy ----
eval_spatial:
  num_sample_frames: 20
  vlm_model: gemini-3-flash-preview

# ---- Stage 7: Cross-Site ----
eval_crosssite:
  num_clips_per_model: 30
  vlm_model: gemini-3-flash-preview

# ---- Hardware / Cloud ----
cloud:
  provider: runpod
  gpu_type: H100
  num_gpus: 1
  max_cost_usd: 500
  auto_shutdown: true
